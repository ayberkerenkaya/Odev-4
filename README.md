# ğŸ–§ Softmax Load Balancer

> Final DeÄŸerlendirme Ã–devi - 1  
> Client-Side Load Balancer: Softmax Action Selection

---

## ğŸ“Œ Proje HakkÄ±nda

Bu proje, **K adet sunucudan** oluÅŸan bir kÃ¼meye gelen istekleri akÄ±llÄ±ca daÄŸÄ±tan bir **istemci taraflÄ± yÃ¼k dengeleyici** simÃ¼lasyonudur.

Klasik Round-Robin veya Random algoritmalarÄ±nÄ±n aksine, bu yÃ¼k dengeleyici **geÃ§miÅŸ performans verisine** dayanarak Ã¶ÄŸrenir ve zamanla en hÄ±zlÄ± sunucuyu daha sÄ±k seÃ§meye baÅŸlar.

---

## ğŸ§  Temel Kavramlar

| Kavram | AÃ§Ä±klama |
|--------|----------|
| **Action (Eylem)** | Her istek iÃ§in hangi sunucunun seÃ§ileceÄŸi |
| **Reward (Ã–dÃ¼l)** | `-latency` â†’ gecikme dÃ¼ÅŸtÃ¼kÃ§e Ã¶dÃ¼l artar |
| **Q[k]** | k. sunucunun tahmini performans skoru |
| **Softmax** | Q deÄŸerlerine gÃ¶re olasÄ±lÄ±ksal sunucu seÃ§imi |
| **Temperature (Ï„)** | KeÅŸif ve sÃ¶mÃ¼rÃ¼ arasÄ±ndaki denge |
| **Alpha (Î±)** | Ã–ÄŸrenme hÄ±zÄ± |

---

## âš™ï¸ AlgoritmanÄ±n Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±

```
Her istek iÃ§in:
  1. Softmax ile sunucu seÃ§       â†’ P(k) = e^(Q[k]/Ï„) / Î£ e^(Q[j]/Ï„)
  2. Gecikmeyi Ã¶lÃ§                â†’ gÃ¼rÃ¼ltÃ¼lÃ¼ gÃ¶zlem
  3. Ã–dÃ¼lÃ¼ hesapla                â†’ reward = -latency
  4. Q deÄŸerini gÃ¼ncelle          â†’ Q[k] = Q[k] + Î± * (reward - Q[k])
  5. OrtamÄ± kaydÄ±r (drift)        â†’ non-stationary simÃ¼lasyonu
```

---

## ğŸŒ¡ï¸ Temperature (Ï„) Parametresi

```
Ï„ bÃ¼yÃ¼k (Ã¶rn: 50) â†’ TÃ¼m sunucular benzer olasÄ±lÄ±kla seÃ§ilir â†’ Daha fazla keÅŸif
Ï„ kÃ¼Ã§Ã¼k (Ã¶rn: 1)  â†’ En iyi sunucu neredeyse hep seÃ§ilir    â†’ Daha aÃ§gÃ¶zlÃ¼
```

---

## ğŸ“ Proje YapÄ±sÄ±

```
Main.java
â”œâ”€â”€ main()                  â†’ Ana simÃ¼lasyon dÃ¶ngÃ¼sÃ¼
â”œâ”€â”€ initialize()            â†’ Sunuculara baÅŸlangÄ±Ã§ deÄŸeri ata
â”œâ”€â”€ softmaxSelect()         â†’ OlasÄ±lÄ±ksal sunucu seÃ§imi
â”œâ”€â”€ softmaxProbabilities()  â†’ Softmax formÃ¼lÃ¼ hesabÄ±
â”œâ”€â”€ observe()               â†’ GÃ¼rÃ¼ltÃ¼lÃ¼ gecikme Ã¶lÃ§Ã¼mÃ¼
â”œâ”€â”€ updateQ()               â†’ Q deÄŸerini gÃ¼ncelle (Ã¶ÄŸrenme)
â”œâ”€â”€ drift()                 â†’ Non-stationary ortam simÃ¼lasyonu
â”œâ”€â”€ printProgress()         â†’ Ara sonuÃ§larÄ± yazdÄ±r
â””â”€â”€ printFinalResults()     â†’ Nihai sonuÃ§larÄ± yazdÄ±r
```

---

## ğŸš€ NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r

```bash
# Derle
javac Main.java

# Ã‡alÄ±ÅŸtÄ±r
java Main
```

---

## ğŸ”§ Parametreler

```java
static final int    K         = 4;     // Sunucu sayÄ±sÄ±
static final int    STEPS     = 3000;  // Toplam istek sayÄ±sÄ±
static final double ALPHA     = 0.1;   // Ã–ÄŸrenme hÄ±zÄ±
static final double TAU       = 20.0;  // Softmax temperature
static final double DRIFT_STD = 1.0;   // Gecikme kayma miktarÄ±
static final double NOISE_STD = 10.0;  // Ã–lÃ§Ã¼m gÃ¼rÃ¼ltÃ¼sÃ¼
```

---

## ğŸ“Š Ã–rnek Ã‡Ä±ktÄ±

```
=== Softmax Load Balancer SimÃ¼lasyonu ===

BaÅŸlangÄ±Ã§ gerÃ§ek gecikmeleri (ms):
  Server-0: 127.3 ms
  Server-1: 73.6 ms
  Server-2: 189.4 ms
  Server-3: 95.1 ms

--- AdÄ±m 500 | Ortalama Gecikme: 112.4 ms ---
  Server-0 | Gecikme tahmini: 128.1 ms | OlasÄ±lÄ±k: 18.3% | SeÃ§ilme: 121
  Server-1 | Gecikme tahmini:  74.2 ms | OlasÄ±lÄ±k: 42.7% | SeÃ§ilme: 198
  Server-2 | Gecikme tahmini: 190.6 ms | OlasÄ±lÄ±k:  8.1% | SeÃ§ilme: 87
  Server-3 | Gecikme tahmini:  96.3 ms | OlasÄ±lÄ±k: 30.9% | SeÃ§ilme: 94

==========================================
  SONUÃ‡LAR
==========================================
  Ortalama Gecikme: 98.72 ms

  Sunucu KullanÄ±m DaÄŸÄ±lÄ±mÄ±:
    Server-0:  412 istek (13.7%)
    Server-1: 1538 istek (51.3%)
    Server-2:  287 istek ( 9.6%)
    Server-3:  763 istek (25.4%)
```

---

## ğŸ“š Non-Stationary Ortam Nedir?

GerÃ§ek hayatta sunucu gecikmeleri sabit kalmaz. Ã–ÄŸle saatinde trafik artar, gece dÃ¼ÅŸer. Bu projedeki `drift()` metodu bunu simÃ¼le eder â€” her adÄ±mda gerÃ§ek gecikmeler kÃ¼Ã§Ã¼k miktarda deÄŸiÅŸir. Bu yÃ¼zden **sabit ortalama** yerine **aÄŸÄ±rlÄ±klÄ± gÃ¼ncelleme** kullanÄ±lÄ±r:

```
Q[k] â† Q[k] + Î± Ã— (reward - Q[k])
```

`Î± = 0.1` â†’ Yeni gÃ¶zleme %10, geÃ§miÅŸe %90 aÄŸÄ±rlÄ±k verilir.

---

## ğŸ”— Ä°lgili Kavramlar

- Multi-Armed Bandit Problem
- Reinforcement Learning
- Temporal Difference Learning
- Exploration vs Exploitation Tradeoff
